{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d: Error no file named diffusion_pytorch_model.safetensors found in directory /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d: Error no file named diffusion_pytorch_model.safetensors found in directory /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d: Error no file named diffusion_pytorch_model.safetensors found in directory /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d: Error no file named diffusion_pytorch_model.safetensors found in directory /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils import load_epsilon_net, load_image\n",
    "from utils import load_epsilon_net\n",
    "from sampling.dps import dps, dps_save\n",
    "from sampling.dps_dpms import dps_dpms_save\n",
    "from sampling.dmps import dpms_save\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import display_image\n",
    "import os\n",
    "import math\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from evaluation.perception import LPIPS\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "def make_gif(frame_folder, n_steps):\n",
    "    frames = [Image.open(image) for image in sorted(glob.glob(f\"{frame_folder}/*.png\"))[::-1]]\n",
    "    print(frame_folder)\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(frame_folder+\"/output.gif\", format=\"GIF\", append_images=frames,\n",
    "               save_all=True, duration=300, loop=0)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "#n_steps = 100\n",
    "torch.set_default_device(device)\n",
    "names = [\"Rajit\", \"Dolly\", \"Marco\", \"Bernardin\"]\n",
    "\n",
    "for name in names:\n",
    "    img_path = f\"./hackathon_starter_kit/material/{name}.png\"\n",
    "    x_origin = load_image(img_path, device = device, resize = (256, 256))\n",
    "    if x_origin.shape[0] == 4:\n",
    "        x_origin = x_origin[:3, :, :]\n",
    "\n",
    "\n",
    "    # load the degradation operator\n",
    "    #path_operator = f\"./material/degradation_operators/sr16.pt\"\n",
    "    path_operator = f\"./hackathon_starter_kit/material/degradation_operators/sr16.pt\"\n",
    "    degradation_operator = torch.load(path_operator, map_location=device)\n",
    "\n",
    "    # apply degradation operator\n",
    "    y = degradation_operator.H(x_origin[None])\n",
    "    y = y.squeeze(0)\n",
    "\n",
    "    sigma = [0.01]\n",
    "    n_steps = [100]\n",
    "\n",
    "\n",
    "    methods = [\"dps\", \"dps_dpms\"]\n",
    "\n",
    "    output_base= f\"./output/sr16_ood_{name}\"\n",
    "    lpips = LPIPS()\n",
    "\n",
    "    for n in n_steps:\n",
    "        for s in sigma:\n",
    "            K = [int(n/10)]\n",
    "            for k in K:\n",
    "\n",
    "                # add noise\n",
    "                y = y + s * torch.randn_like(y)\n",
    "\n",
    "                # define inverse problem\n",
    "                inverse_problem = (y, degradation_operator, s)\n",
    "\n",
    "                # load model\n",
    "                eps_net = load_epsilon_net(\"celebahq\", n, device)\n",
    "\n",
    "                # solve problem\n",
    "                initial_noise = torch.randn((1, 3, 256, 256), device=device)\n",
    "\n",
    "\n",
    "                #make output dir\n",
    "                if \"dps\" in methods:\n",
    "                    seed = 2024\n",
    "                    torch.manual_seed(seed=seed) # for reproducibility\n",
    "\n",
    "                    output_dir = os.path.join(output_base, f\"dps_n_step={n}_sigma={s}/progress\")\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    time_dps = time.time()\n",
    "                    reconstruction_dps = dps_save(initial_noise, inverse_problem, eps_net, output_path=output_dir, interval=1)\n",
    "                    time_dps =  time.time() - time_dps\n",
    "\n",
    "                if \"dps_dpms\" in methods:\n",
    "                    seed = 2024\n",
    "                    torch.manual_seed(seed=seed) # for reproducibility\n",
    "\n",
    "                    output_dir = os.path.join(output_base, f\"dps_dpms_n_step={n}_sigma={s}/progress\")\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    time_dps_dpms = time.time()\n",
    "                    reconstruction_dps_dpms = dps_dpms_save(initial_noise, inverse_problem, eps_net, lam = 1, k = k, output_path=output_dir, interval=100)\n",
    "                    time_dps_dpms = time.time() - time_dps_dpms\n",
    "                if \"dpms\" in methods:\n",
    "                    seed = 2024\n",
    "                    torch.manual_seed(seed=seed) # for reproducibility\n",
    "\n",
    "                    output_dir = os.path.join(output_base, f\"dpms_n_step={n}_sigma={s}\")\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    reconstruction_dpms = dpms_save(initial_noise, inverse_problem, eps_net, k, output_path=output_dir, interval=100)\n",
    "                    \n",
    "\n",
    "                # make_gif(output_dir, n_steps)\n",
    "                \n",
    "                if method == \"sr\":\n",
    "\n",
    "                    n_channels = 3\n",
    "                    n_pixel_per_channel = y.shape[0] // n_channels\n",
    "                    hight = width = int(math.sqrt(n_pixel_per_channel))\n",
    "\n",
    "                    y_reshaped = y.reshape(n_channels, hight, width)\n",
    "\n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    y_reshaped =  -torch.ones(3 * 256 * 256, device=device)\n",
    "                    y_reshaped[: y.shape[0]] = y\n",
    "                    y_reshaped = degradation_operator.V(y_reshaped[None])\n",
    "                    y_reshaped = y_reshaped.reshape(3, 256, 256)\n",
    "\n",
    "                fig, axes = plt.subplots(1, 4, figsize = (20, 20))\n",
    "\n",
    "                images = (x_origin, y_reshaped, reconstruction_dps[0], reconstruction_dps_dpms[0])\n",
    "                titles = (\"original\", \"degraded\", \"DPS\", \"DPS-DMPS\")\n",
    "\n",
    "                # display figures\n",
    "                \n",
    "                for ax, img, title in zip(axes, images,titles):\n",
    "                    display_image(img, ax)\n",
    "                    ax.set_title(title, fontsize = 25)\n",
    "                    ax.set_axis_off() \n",
    "                    if title == \"DPS\":\n",
    "                        psnr_dps = round(psnr(x_origin.cpu().numpy(), reconstruction_dps[0].cpu().numpy()), 3)\n",
    "                        lpips_dps = round(lpips.score(x_origin, reconstruction_dps[0].clamp(-1, 1)).item(), 3)\n",
    "                        ax.text(10, 280, \"PSNR:\"+str(psnr_dps)+\"dB\", fontsize=21, color = (0,0,0))           \n",
    "                        ax.text(10, 300, \"LPIPS:\"+str(lpips_dps), fontsize=21, color = (0,0,0))  \n",
    "                        ax.text(10 ,320, \"Time:\"+str(round(time_dps, 3))+ \"s\", fontsize=21, color = (0,0,0))           \n",
    "\n",
    "                    elif title == \"DPS-DMPS\":\n",
    "                        psnr_dps_dpms = round(psnr(x_origin.cpu().numpy(), reconstruction_dps_dpms[0].cpu().numpy()), 3)\n",
    "                        lpips_dps_dpms =  round(lpips.score(x_origin, reconstruction_dps_dpms[0].clamp(-1, 1)).item(), 3)\n",
    "                        ax.text(10, 280, \"PSNR:\"+str(psnr_dps_dpms)+\"dB\", fontsize=21, color = (0,0,0))           \n",
    "                        ax.text(10 ,300, \"LPIPS:\"+str(lpips_dps_dpms), fontsize=21, color = (0,0,0)) \n",
    "                        ax.text(10 ,320, \"Time:\"+str(round(time_dps_dpms, 3))+ \"s\", fontsize=21, color = (0,0,0))           \n",
    "\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(output_dir+f\"/output_n_step={n}_sigma={s}.png\", bbox_inches = \"tight\")\n",
    "\n",
    "                plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
