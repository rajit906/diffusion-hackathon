{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d: Error no file named diffusion_pytorch_model.safetensors found in directory /home/obanmarcos/.cache/huggingface/hub/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/obanmarcos/PhD/Projects/GM Hackathon/hackathon_starter_kit/venv-hackathon/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "./output/inpainting_middle_indist/dps_dpms_n_step=100_sigma=0.01/progress\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils import load_epsilon_net, load_image\n",
    "from utils import load_epsilon_net\n",
    "from sampling.dps import dps, dps_save\n",
    "from sampling.dps_dpms import dps_dpms_save\n",
    "from sampling.dmps import dpms_save\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import display_image\n",
    "import os\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from evaluation.perception import LPIPS\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "def make_gif(frame_folder, n_steps):\n",
    "    frames = [Image.open(image) for image in sorted(glob.glob(f\"{frame_folder}/*.png\"))[::-1]]\n",
    "    print(frame_folder)\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(frame_folder+\"/output.gif\", format=\"GIF\", append_images=frames,\n",
    "               save_all=True, duration=300, loop=0)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "#n_steps = 100\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "img_path = \"./hackathon_starter_kit/material/celebahq_img/00010.jpg\"\n",
    "x_origin = load_image(img_path, device = device, resize = (256, 256))\n",
    "if x_origin.shape[0] == 4:\n",
    "    x_origin = x_origin[:3, :, :]\n",
    "\n",
    "\n",
    "# load the degradation operator\n",
    "#path_operator = f\"./material/degradation_operators/sr16.pt\"\n",
    "path_operator = f\"./hackathon_starter_kit/material/degradation_operators/inpainting_middle.pt\"\n",
    "degradation_operator = torch.load(path_operator, map_location=device)\n",
    "\n",
    "# apply degradation operator\n",
    "y = degradation_operator.H(x_origin[None])\n",
    "y = y.squeeze(0)\n",
    "\n",
    "sigma = [0.01]\n",
    "n_steps = [100]\n",
    "\n",
    "\n",
    "methods = [\"dps\", \"dps_dpms\"]\n",
    "\n",
    "output_base= \"./output/inpainting_middle_indist\"\n",
    "lpips = LPIPS()\n",
    "\n",
    "for n in n_steps:\n",
    "    for s in sigma:\n",
    "        K = [int(n/10)]\n",
    "        for k in K:\n",
    "\n",
    "            # add noise\n",
    "            y = y + s * torch.randn_like(y)\n",
    "\n",
    "            # define inverse problem\n",
    "            inverse_problem = (y, degradation_operator, s)\n",
    "\n",
    "            # load model\n",
    "            eps_net = load_epsilon_net(\"celebahq\", n, device)\n",
    "\n",
    "            # solve problem\n",
    "            initial_noise = torch.randn((1, 3, 256, 256), device=device)\n",
    "\n",
    "\n",
    "            #make output dir\n",
    "            if \"dps\" in methods:\n",
    "                seed = 2024\n",
    "                torch.manual_seed(seed=seed) # for reproducibility\n",
    "\n",
    "                output_dir = os.path.join(output_base, f\"dps_n_step={n}_sigma={s}/progress\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                reconstruction_dps = dps_save(initial_noise, inverse_problem, eps_net, output_path=output_dir, interval=1)\n",
    "            if \"dps_dpms\" in methods:\n",
    "                seed = 2024\n",
    "                torch.manual_seed(seed=seed) # for reproducibility\n",
    "\n",
    "                output_dir = os.path.join(output_base, f\"dps_dpms_n_step={n}_sigma={s}/progress\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                reconstruction_dps_dpms = dps_dpms_save(initial_noise, inverse_problem, eps_net, lam = 1, k = 10, output_path=output_dir, interval=1)\n",
    "            if \"dpms\" in methods:\n",
    "                seed = 2024\n",
    "                torch.manual_seed(seed=seed) # for reproducibility\n",
    "\n",
    "                output_dir = os.path.join(output_base, f\"dpms_n_step={n}_sigma={s}\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                reconstruction_dpms = dpms_save(initial_noise, inverse_problem, eps_net, k, output_path=output_dir, interval=1)\n",
    "            \n",
    "            make_gif(output_dir, n_steps)\n",
    "\n",
    "            y_reshaped =  -torch.ones(3 * 256 * 256, device=device)\n",
    "            y_reshaped[: y.shape[0]] = y\n",
    "            y_reshaped = degradation_operator.V(y_reshaped[None])\n",
    "            y_reshaped = y_reshaped.reshape(3, 256, 256)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 4, figsize = (20, 20))\n",
    "\n",
    "            images = (x_origin, y_reshaped, reconstruction_dps[0], reconstruction_dps_dpms[0])\n",
    "            titles = (\"original\", \"degraded\", \"DPS\", \"DPS-DMPS\")\n",
    "\n",
    "            # display figures\n",
    "            \n",
    "            for ax, img, title in zip(axes, images,titles):\n",
    "                display_image(img, ax)\n",
    "                ax.set_title(title, fontsize = 25)\n",
    "                ax.set_axis_off() \n",
    "                if title == \"DPS\":\n",
    "                    psnr_dps = round(psnr(x_origin.cpu().numpy(), reconstruction_dps[0].cpu().numpy()), 3)\n",
    "                    lpips_dps = round(lpips.score(x_origin, reconstruction_dps[0].clamp(-1, 1)).item(), 3)\n",
    "                    ax.text(10, 280, \"PSNR:\"+str(psnr_dps)+\"dB\", fontsize=21, color = (0,0,0))           \n",
    "                    ax.text(10, 300, \"LPIPS:\"+str(lpips_dps), fontsize=21, color = (0,0,0))  \n",
    "\n",
    "                elif title == \"DPS-DMPS\":\n",
    "                    psnr_dps_dpms = round(psnr(x_origin.cpu().numpy(), reconstruction_dps_dpms[0].cpu().numpy()), 3)\n",
    "                    lpips_dps_dpms =  round(lpips.score(x_origin, reconstruction_dps_dpms[0].clamp(-1, 1)).item(), 3)\n",
    "                    ax.text(10, 280, \"PSNR:\"+str(psnr_dps_dpms)+\"dB\", fontsize=21, color = (0,0,0))           \n",
    "                    ax.text(10 ,300, \"LPIPS:\"+str(lpips_dps_dpms), fontsize=21, color = (0,0,0))           \n",
    "\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(output_dir+f\"/output_n_step={n}_sigma={s}.png\", bbox_inches = \"tight\")\n",
    "\n",
    "            plt.close(fig)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
